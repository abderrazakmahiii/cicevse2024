{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ee43afb4-08f2-406d-a49e-f7f1d55bd31e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the dataset\n",
      "Dataset Loaded\n",
      "Number of columns in the dataset: 911\n",
      "Dropped 685 constant value columns.\n",
      "Number of columns in the dataset: 226\n",
      "Dropped columns irrelevant for scenario prediction: ['time', 'Attack', 'Label', 'interface']\n",
      "Number of columns in the dataset: 222\n",
      "\n",
      "Encoding for column 'State':\n",
      "'Charging' -> 0 (count: 3584)\n",
      "'idle' -> 1 (count: 2582)\n",
      "\n",
      "Encoding for column 'Scenario':\n",
      "'Benign' -> 0 (count: 2302)\n",
      "'Cryptojacking' -> 1 (count: 1793)\n",
      "'DoS' -> 2 (count: 865)\n",
      "'Recon' -> 3 (count: 1206)\n",
      "\n",
      "--- Balancing the dataset for 'Scenario' based on the minimum 'Idle' count ---\n",
      "\n",
      "Processing Scenario: Benign\n",
      "  Balanced counts - Idle: 365, Charging: 365, Total: 730\n",
      "\n",
      "Processing Scenario: Cryptojacking\n",
      "  Balanced counts - Idle: 365, Charging: 365, Total: 730\n",
      "\n",
      "Processing Scenario: DoS\n",
      "  Balanced counts - Idle: 365, Charging: 365, Total: 730\n",
      "\n",
      "Processing Scenario: Recon\n",
      "  Balanced counts - Idle: 365, Charging: 365, Total: 730\n",
      "\n",
      "--- Splitting data into Training and Testing Sets with Constraints ---\n",
      "\n",
      "--- Training Set Information ---\n",
      "Number of rows in Training Set: 2336\n",
      "\n",
      "Training Set 'Scenario' Value Counts:\n",
      "Scenario\n",
      "Benign           584\n",
      "Cryptojacking    584\n",
      "DoS              584\n",
      "Recon            584\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Training Set 'State' Counts per 'Scenario':\n",
      "Scenario: Benign - Idle: 292, Charging: 292\n",
      "Scenario: Cryptojacking - Idle: 292, Charging: 292\n",
      "Scenario: DoS - Idle: 292, Charging: 292\n",
      "Scenario: Recon - Idle: 292, Charging: 292\n",
      "\n",
      "--- Testing Set Information ---\n",
      "Number of rows in Testing Set: 584\n",
      "\n",
      "Testing Set 'Scenario' Value Counts:\n",
      "Scenario\n",
      "Benign           146\n",
      "Cryptojacking    146\n",
      "DoS              146\n",
      "Recon            146\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Testing Set 'State' Counts per 'Scenario':\n",
      "Scenario: Benign - Idle: 73, Charging: 73\n",
      "Scenario: Cryptojacking - Idle: 73, Charging: 73\n",
      "Scenario: DoS - Idle: 73, Charging: 73\n",
      "Scenario: Recon - Idle: 73, Charging: 73\n",
      "\n",
      "--- Defining Features (X) and Target (y) for Training Set ---\n",
      "Shape of X_train: (2336, 221)\n",
      "Shape of y_train: (2336,)\n",
      "\n",
      "--- Defining Features (X) and Target (y) for Testing Set ---\n",
      "Shape of X_test: (584, 221)\n",
      "Shape of y_test: (584,)\n",
      "\n",
      "--- Identifying and Scaling Numerical Features for Training Set ---\n",
      "Numerical features scaled for training set.\n",
      "\n",
      "--- Identifying and Scaling Numerical Features for Testing Set ---\n",
      "Numerical features scaled for testing set.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1269/1045238614.py:52: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  min_idle_samples = scenario_groups.apply(lambda x: x[x['State'] == idle_encoded].shape[0]).min()\n"
     ]
    }
   ],
   "source": [
    "# Preprocessing\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 1. Load dataset\n",
    "print(\"Loading the dataset\")\n",
    "data = pd.read_csv(\"Kernel_Events_1.csv\")\n",
    "print(\"Dataset Loaded\")\n",
    "\n",
    "# 2. Display the number of columns\n",
    "print(\"Number of columns in the dataset:\", len(data.columns))\n",
    "\n",
    "# 3. Drop columns with a single unique value\n",
    "single_unique_cols = data.columns[data.nunique() == 1]\n",
    "data = data.drop(columns=single_unique_cols)\n",
    "print(f\"Dropped {len(single_unique_cols)} constant value columns.\")\n",
    "\n",
    "# 4. Display the number of columns\n",
    "print(\"Number of columns in the dataset:\", len(data.columns))\n",
    "\n",
    "# 5. Drop irrelevant columns for 'Scenario' prediction\n",
    "columns_to_drop = ['time', 'Attack', 'Label', 'interface']\n",
    "data = data.drop(columns=columns_to_drop)\n",
    "print(f\"Dropped columns irrelevant for scenario prediction: {columns_to_drop}\")\n",
    "print(\"Number of columns in the dataset:\", len(data.columns))\n",
    "\n",
    "# 6. Encode categorical features 'State' and 'Scenario'\n",
    "encoded_data = data.copy()\n",
    "encoders = {}\n",
    "columns_to_encode = ['State', 'Scenario']\n",
    "\n",
    "for col in columns_to_encode:\n",
    "    le = LabelEncoder()\n",
    "    encoded_data[col] = le.fit_transform(encoded_data[col])\n",
    "    encoders[col] = le\n",
    "    value_counts = data[col].value_counts()\n",
    "    print(f\"\\nEncoding for column '{col}':\")\n",
    "    for original, encoded in zip(le.classes_, le.transform(le.classes_)):\n",
    "        count = value_counts[original]\n",
    "        print(f\"'{original}' -> {encoded} (count: {count})\")\n",
    "\n",
    "# --- Strictly Balance the dataset for 'Scenario' based on the minimum 'Idle' count ---\n",
    "\n",
    "print(\"\\n--- Balancing the dataset for 'Scenario' based on the minimum 'Idle' count ---\")\n",
    "\n",
    "idle_encoded = encoders['State'].transform(['idle'])[0]\n",
    "charging_encoded = encoders['State'].transform(['Charging'])[0]\n",
    "\n",
    "scenario_groups = encoded_data.groupby('Scenario')\n",
    "min_idle_samples = scenario_groups.apply(lambda x: x[x['State'] == idle_encoded].shape[0]).min()\n",
    "balanced_data = []\n",
    "target_samples_per_scenario = 2 * min_idle_samples\n",
    "\n",
    "for scenario, group in scenario_groups:\n",
    "    original_scenario = encoders['Scenario'].inverse_transform([scenario])[0]\n",
    "    print(f\"\\nProcessing Scenario: {original_scenario}\")\n",
    "\n",
    "    idle_group = group[group['State'] == idle_encoded]\n",
    "    charging_group = group[group['State'] == charging_encoded]\n",
    "\n",
    "    idle_sampled = idle_group.sample(n=min(len(idle_group), min_idle_samples), random_state=42)\n",
    "    charging_sampled = charging_group.sample(n=min(len(charging_group), min_idle_samples), random_state=42)\n",
    "\n",
    "    sampled_group = pd.concat([idle_sampled, charging_sampled])\n",
    "\n",
    "    if len(sampled_group) < target_samples_per_scenario:\n",
    "        diff = target_samples_per_scenario - len(sampled_group)\n",
    "        additional_charging = charging_group.drop(charging_sampled.index, errors='ignore').sample(n=min(diff // 2 + diff % 2, len(charging_group) - len(charging_sampled)), random_state=42)\n",
    "        sampled_group = pd.concat([sampled_group, additional_charging])\n",
    "        additional_idle = idle_group.drop(idle_sampled.index, errors='ignore').sample(n=min(diff // 2, len(idle_group) - len(idle_sampled)), random_state=42)\n",
    "        sampled_group = pd.concat([sampled_group, additional_idle])\n",
    "    elif len(sampled_group) > target_samples_per_scenario:\n",
    "        sampled_group = sampled_group.sample(n=target_samples_per_scenario, random_state=42)\n",
    "\n",
    "    balanced_data.append(sampled_group)\n",
    "    print(f\"  Balanced counts - Idle: {len(sampled_group[sampled_group['State'] == idle_encoded])}, Charging: {len(sampled_group[sampled_group['State'] == charging_encoded])}, Total: {len(sampled_group)}\")\n",
    "\n",
    "balanced_df = pd.concat(balanced_data).sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# --- Split data into training and testing sets with constraints ---\n",
    "\n",
    "print(\"\\n--- Splitting data into Training and Testing Sets with Constraints ---\")\n",
    "\n",
    "train_data = []\n",
    "test_data = []\n",
    "test_size = 0.2\n",
    "random_state = 42\n",
    "\n",
    "for scenario_code in balanced_df['Scenario'].unique():\n",
    "    scenario_df = balanced_df[balanced_df['Scenario'] == scenario_code].copy()\n",
    "    n_samples = len(scenario_df)\n",
    "    train_n = int(n_samples * (1 - test_size))\n",
    "\n",
    "    # Ensure equal number of Idle and Charging in training\n",
    "    idle_df = scenario_df[scenario_df['State'] == idle_encoded]\n",
    "    charging_df = scenario_df[scenario_df['State'] == charging_encoded]\n",
    "\n",
    "    train_idle_n = int(train_n * 0.5)\n",
    "    train_charging_n = train_n - train_idle_n\n",
    "\n",
    "    train_idle_sampled = idle_df.sample(n=min(len(idle_df), train_idle_n), random_state=random_state)\n",
    "    train_charging_sampled = charging_df.sample(n=min(len(charging_df), train_charging_n), random_state=random_state)\n",
    "    train_scenario = pd.concat([train_idle_sampled, train_charging_sampled])\n",
    "    train_data.append(train_scenario)\n",
    "\n",
    "    test_scenario = scenario_df.drop(train_scenario.index)\n",
    "    test_data.append(test_scenario)\n",
    "\n",
    "train_df = pd.concat(train_data).sample(frac=1, random_state=random_state).reset_index(drop=True)\n",
    "test_df = pd.concat(test_data).sample(frac=1, random_state=random_state).reset_index(drop=True)\n",
    "\n",
    "# --- Display Training Set Information ---\n",
    "\n",
    "print(\"\\n--- Training Set Information ---\")\n",
    "print(\"Number of rows in Training Set:\", len(train_df))\n",
    "print(\"\\nTraining Set 'Scenario' Value Counts:\")\n",
    "train_scenario_counts = train_df['Scenario'].value_counts().sort_index().rename(index=lambda x: encoders['Scenario'].inverse_transform([x])[0])\n",
    "print(train_scenario_counts)\n",
    "\n",
    "print(\"\\nTraining Set 'State' Counts per 'Scenario':\")\n",
    "train_grouped = train_df.groupby('Scenario')\n",
    "for scenario, group in train_grouped:\n",
    "    original_scenario = encoders['Scenario'].inverse_transform([scenario])[0]\n",
    "    idle_count = group[group['State'] == idle_encoded].shape[0]\n",
    "    charging_count = group[group['State'] == charging_encoded].shape[0]\n",
    "    print(f\"Scenario: {original_scenario} - Idle: {idle_count}, Charging: {charging_count}\")\n",
    "\n",
    "# --- Display Testing Set Information ---\n",
    "\n",
    "print(\"\\n--- Testing Set Information ---\")\n",
    "print(\"Number of rows in Testing Set:\", len(test_df))\n",
    "print(\"\\nTesting Set 'Scenario' Value Counts:\")\n",
    "test_scenario_counts = test_df['Scenario'].value_counts().sort_index().rename(index=lambda x: encoders['Scenario'].inverse_transform([x])[0])\n",
    "print(test_scenario_counts)\n",
    "\n",
    "print(\"\\nTesting Set 'State' Counts per 'Scenario':\")\n",
    "test_grouped = test_df.groupby('Scenario')\n",
    "for scenario, group in test_grouped:\n",
    "    original_scenario = encoders['Scenario'].inverse_transform([scenario])[0]\n",
    "    idle_count = group[group['State'] == idle_encoded].shape[0]\n",
    "    charging_count = group[group['State'] == charging_encoded].shape[0]\n",
    "    print(f\"Scenario: {original_scenario} - Idle: {idle_count}, Charging: {charging_count}\")\n",
    "\n",
    "# --- Define Features (X) and Target (y) for both sets ---\n",
    "\n",
    "print(\"\\n--- Defining Features (X) and Target (y) for Training Set ---\")\n",
    "X_train = train_df.drop(columns=['Scenario'])\n",
    "y_train = train_df['Scenario']\n",
    "print(\"Shape of X_train:\", X_train.shape)\n",
    "print(\"Shape of y_train:\", y_train.shape)\n",
    "\n",
    "print(\"\\n--- Defining Features (X) and Target (y) for Testing Set ---\")\n",
    "X_test = test_df.drop(columns=['Scenario'])\n",
    "y_test = test_df['Scenario']\n",
    "print(\"Shape of X_test:\", X_test.shape)\n",
    "print(\"Shape of y_test:\", y_test.shape)\n",
    "\n",
    "# --- Identify and scale numerical features for both sets ---\n",
    "\n",
    "print(\"\\n--- Identifying and Scaling Numerical Features for Training Set ---\")\n",
    "numerical_cols_train = X_train.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "scaler = StandardScaler()\n",
    "if numerical_cols_train:\n",
    "    X_train[numerical_cols_train] = scaler.fit_transform(X_train[numerical_cols_train])\n",
    "    print(\"Numerical features scaled for training set.\")\n",
    "else:\n",
    "    print(\"No numerical features to scale in training set.\")\n",
    "\n",
    "print(\"\\n--- Identifying and Scaling Numerical Features for Testing Set ---\")\n",
    "numerical_cols_test = X_test.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "if numerical_cols_test:\n",
    "    X_test[numerical_cols_test] = scaler.transform(X_test[numerical_cols_test]) \n",
    "    print(\"Numerical features scaled for testing set.\")\n",
    "else:\n",
    "    print(\"No numerical features to scale in testing set.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0725b020-4e71-4b68-ad41-6cef0717715f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Random Forest ---\n",
      "Accuracy: 0.9675\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "       Benign       0.97      1.00      0.99       146\n",
      "Cryptojacking       1.00      1.00      1.00       146\n",
      "          DoS       0.93      0.95      0.94       146\n",
      "        Recon       0.96      0.92      0.94       146\n",
      "\n",
      "     accuracy                           0.97       584\n",
      "    macro avg       0.97      0.97      0.97       584\n",
      " weighted avg       0.97      0.97      0.97       584\n",
      "\n",
      "\n",
      "--- SVM ---\n",
      "Accuracy: 0.9623\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "       Benign       0.95      1.00      0.98       146\n",
      "Cryptojacking       1.00      1.00      1.00       146\n",
      "          DoS       0.93      0.95      0.94       146\n",
      "        Recon       0.96      0.90      0.93       146\n",
      "\n",
      "     accuracy                           0.96       584\n",
      "    macro avg       0.96      0.96      0.96       584\n",
      " weighted avg       0.96      0.96      0.96       584\n",
      "\n",
      "\n",
      "--- Logistic Regression ---\n",
      "Accuracy: 0.9795\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "       Benign       0.97      1.00      0.98       146\n",
      "Cryptojacking       1.00      1.00      1.00       146\n",
      "          DoS       1.00      0.93      0.96       146\n",
      "        Recon       0.95      0.99      0.97       146\n",
      "\n",
      "     accuracy                           0.98       584\n",
      "    macro avg       0.98      0.98      0.98       584\n",
      " weighted avg       0.98      0.98      0.98       584\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Store models and their names\n",
    "models = {\n",
    "    \"Random Forest\": RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    \"SVM\": SVC(kernel='rbf', probability=True, random_state=42),\n",
    "    \"Logistic Regression\": LogisticRegression(multi_class='multinomial', solver='lbfgs', max_iter=1000, random_state=42)\n",
    "}\n",
    "\n",
    "# Train and evaluate each model\n",
    "for name, model in models.items():\n",
    "    print(f\"\\n--- {name} ---\")\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    print(f\"Accuracy: {acc:.4f}\")\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(y_test, y_pred, target_names=encoders['Scenario'].classes_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cdf986ce-2714-4353-a79b-9f9734a33d0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Running t-test comparison: Benign vs Each Attack Scenario ---\n",
      "\n",
      "Comparing Benign vs Cryptojacking:\n",
      "  Significant features (p < 0.05): 136 / 221\n",
      "\n",
      "Comparing Benign vs DoS:\n",
      "  Significant features (p < 0.05): 148 / 221\n",
      "\n",
      "Comparing Benign vs Recon:\n",
      "  Significant features (p < 0.05): 174 / 221\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import ttest_ind\n",
    "import pandas as pd\n",
    "\n",
    "def compare_scenarios_vs_benign(df, feature_columns, scenario_column='Scenario', scenario_encoder=None, alpha=0.05):\n",
    "    \"\"\"\n",
    "    Compares each attack scenario to 'Benign' using t-tests across all features.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): Encoded and preprocessed dataset.\n",
    "        feature_columns (list): List of numerical feature names.\n",
    "        scenario_column (str): Name of the column containing scenario labels.\n",
    "        scenario_encoder (LabelEncoder): Encoder to map scenario names.\n",
    "        alpha (float): Significance level for the t-test.\n",
    "\n",
    "    Returns:\n",
    "        dict: Mapping from scenario name to its t-test results vs. Benign.\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    benign_code = scenario_encoder.transform(['Benign'])[0]\n",
    "    benign_df = df[df[scenario_column] == benign_code]\n",
    "\n",
    "    for scenario_name in scenario_encoder.classes_:\n",
    "        if scenario_name == 'Benign':\n",
    "            continue\n",
    "        scenario_code = scenario_encoder.transform([scenario_name])[0]\n",
    "        attack_df = df[df[scenario_column] == scenario_code]\n",
    "\n",
    "        print(f\"\\nComparing Benign vs {scenario_name}:\")\n",
    "\n",
    "        scenario_results = []\n",
    "        for feature in feature_columns:\n",
    "            try:\n",
    "                t_stat, p_val = ttest_ind(benign_df[feature], attack_df[feature], equal_var=False)\n",
    "                scenario_results.append({\n",
    "                    'feature': feature,\n",
    "                    't_statistic': t_stat,\n",
    "                    'p_value': p_val,\n",
    "                    'significant_difference': p_val < alpha\n",
    "                })\n",
    "            except Exception as e:\n",
    "                scenario_results.append({\n",
    "                    'feature': feature,\n",
    "                    't_statistic': None,\n",
    "                    'p_value': None,\n",
    "                    'significant_difference': False,\n",
    "                    'error': str(e)\n",
    "                })\n",
    "        df_results = pd.DataFrame(scenario_results)\n",
    "        df_results.sort_values(by='p_value', inplace=True)\n",
    "        significant_count = df_results['significant_difference'].sum()\n",
    "        print(f\"  Significant features (p < {alpha}): {significant_count} / {len(feature_columns)}\")\n",
    "        results[scenario_name] = df_results\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "print(\"\\n--- Running t-test comparison: Benign vs Each Attack Scenario ---\")\n",
    "feature_columns = X_train.columns.tolist() \n",
    "scenario_comparisons = compare_scenarios_vs_benign(train_df, feature_columns, scenario_column='Scenario', scenario_encoder=encoders['Scenario'])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
